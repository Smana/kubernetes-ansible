---
# Node Ready: type = ready, status = True
# Node NotReady: type = ready, status = Unknown
- name: See if node is in ready state
  shell: >-
    {{ bin_dir }}/kubectl get node {{ kube_override_hostname|default(inventory_hostname) }}
    -o jsonpath='{ range .status.conditions[?(@.type == "Ready")].status }{ @ }{ end }'
  register: kubectl_node_ready
  delegate_to: "{{ groups['kube-master'][0] }}"
  failed_when: false
  changed_when: false

# SchedulingDisabled: unschedulable = true
# else unschedulable key doesn't exist
- name: See if node is schedulable
  shell: >-
    {{ bin_dir }}/kubectl get node {{ kube_override_hostname|default(inventory_hostname) }}
    -o jsonpath='{ .spec.unschedulable }'
  register: kubectl_node_schedulable
  delegate_to: "{{ groups['kube-master'][0] }}"
  failed_when: false
  changed_when: false

- name: Set if node needs cordoning
  set_fact:
    needs_cordoning: >-
      {% if kubectl_node_ready.stdout == "True" and not kubectl_node_schedulable.stdout -%}
      true
      {%- else -%}
      false
      {%- endif %}

- name: Node draining
  block:
    - name: Cordon node
      command: "{{ bin_dir }}/kubectl cordon {{ kube_override_hostname|default(inventory_hostname) }}"
      delegate_to: "{{ groups['kube-master'][0] }}"

    - name: Check kubectl version
      command: "{{ bin_dir }}/kubectl version --client --short"
      register: kubectl_version
      delegate_to: "{{ groups['kube-master'][0] }}"
      run_once: yes
      changed_when: false
      when:
        - drain_nodes
        - drain_pod_selector

    - name: Ensure minimum version for drain label selector if necessary
      assert:
        that: "kubectl_version.stdout.split(' ')[-1] is version('v1.10.0', '>=')"
      when:
        - drain_nodes
        - drain_pod_selector

    - name: Set drain facts
      set_fact:
        drain_grace_period_final: >-
          {{ (hostvars[groups['kube-master'][0]]['regular_drain_failed'] | default(false))
             | ternary(drain_fallback_grace_period, drain_grace_period) }}
        drain_timeout_final: >-
          {{ (hostvars[groups['kube-master'][0]]['regular_drain_failed'] | default(false))
             | ternary(drain_fallback_timeout, drain_timeout) }}

    - name: Drain node
      command: >-
        {{ bin_dir }}/kubectl drain
        --force
        --ignore-daemonsets
        --grace-period {{ drain_grace_period_final }}
        --timeout {{ drain_timeout_final }}
        --delete-local-data {{ kube_override_hostname|default(inventory_hostname) }}
        {% if drain_pod_selector %}--pod-selector '{{ drain_pod_selector }}'{% endif %}
      when: drain_nodes
      register: result
      failed_when: result.rc != 0 and (not drain_fallback_enabled)

    - name: Set fact regular_drain_failed
      set_fact:
        regular_drain_failed: "{{ result.rc != 0 }}"
      delegate_to: "{{ groups['kube-master'][0] }}"
      run_once: yes
      when: >-
        drain_nodes and
        drain_fallback_enabled and
        (hostvars[groups['kube-master'][0]]['regular_drain_failed'] is not defined)

    - name: Drain node - fallback with disabled eviction
      command: >-
        {{ bin_dir }}/kubectl drain
        --force
        --ignore-daemonsets
        --grace-period {{ drain_fallback_grace_period }}
        --timeout {{ drain_fallback_timeout }}
        --delete-local-data {{ kube_override_hostname|default(inventory_hostname) }}
        {% if drain_pod_selector %}--pod-selector '{{ drain_pod_selector }}'{% endif %}
        --disable-eviction
      when: drain_nodes and drain_fallback_enabled
  rescue:
    - name: Set node back to schedulable
      command: "{{ bin_dir }}/kubectl --kubeconfig /etc/kubernetes/admin.conf uncordon {{ inventory_hostname }}"
    - name: Fail after rescue
      fail:
        msg: "Failed to drain node {{ inventory_hostname }}"
  delegate_to: "{{ groups['kube-master'][0] }}"
  when:
    - needs_cordoning
